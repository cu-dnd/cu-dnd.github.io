---
title: 'Umwelt: Accessible Structured Editing of Multi-Modal Data Representations'
authors:
  - key: jzong
    affiliation: MIT
  - key: ipedraza
  - key: mzc219
  - key: dhajas
  - key: arvindsatya
venue: chi
year: 2024
date: 2024-05-31
doi: 10.1145/3613904.3641996
pdf_url: /publications/umwelt.pdf
html_url: https://dl.acm.org/doi/fullHtml/10.1145/3613904.3641996
themes: [access]
projects: [umwelt]
---

### Abstract

We present Umwelt, an authoring environment for interactive multimodal data representations. In contrast to prior approaches, which center the visual modality, Umwelt treats visualization, sonification, and textual description as coequal representations: they are all derived from a shared abstract data model, such that no modality is prioritized over the others. To simplify specification, Umwelt evaluates a set of heuristics to generate default multimodal representations that express a dataset’s functional relationships. To support smoothly moving between representations, Umwelt maintains a shared query predicate that is reified across all modalities — for instance, navigating the textual description also highlights the visualization and filters the sonification. In a study with 5 blind / low-vision expert users, we found that Umwelt’s multimodal representations afforded complementary overview and detailed perspectives on a dataset, allowing participants to fluidly shift between task- and representation-oriented ways of thinking.
